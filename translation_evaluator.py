import MeCab
from konlpy.tag import Okt, Kkma, Komoran
import sacrebleu
from googletrans import Translator
import torch
import torch.nn.functional as F
from transformers import MarianMTModel, MarianTokenizer
from sentence_transformers import SentenceTransformer
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import numpy as np
import re

# NLTK í•„ìš” ë°ì´í„° ë‹¤ìš´ë¡œë“œ (Ensure this is run once, e.g., at the start of your script or in a setup phase)
try:
    nltk.data.find('tokenizers/punkt')
except nltk.downloader.DownloadError:
    nltk.download('punkt')

# ì–¸ì–´ë³„ í† í¬ë‚˜ì´ì € ì„¤ì • ê°œì„ 
def tokenize_text(original, translated, src_lang, dest_lang):
    # ì†ŒìŠ¤ ì–¸ì–´ í† í°í™”
    if src_lang == "ja":  # ì¼ë³¸ì–´
        mecab = MeCab.Tagger("-Owakati")
        src_tokens = mecab.parse(original).strip().split()
    elif src_lang == "ko":  # í•œêµ­ì–´ - ì—¬ëŸ¬ í† í¬ë‚˜ì´ì € ê²°í•©
        okt = Okt()
        # Komoran and Kkma instances are memory-intensive if created repeatedly.
        # It's better to instantiate them once if possible, or handle their lifecycle.
        # For this script, we'll keep them here for simplicity per function call.
        komoran = Komoran()
        kkma = Kkma()

        # ê° í† í¬ë‚˜ì´ì €ì˜ ê²°ê³¼ë¥¼ ì¡°í•©í•˜ì—¬ ë” ì •í™•í•œ í† í°í™”
        okt_tokens = okt.morphs(original)
        komoran_tokens = komoran.morphs(original)
        kkma_tokens = kkma.morphs(original)

        # ê°€ì¥ ì„¸ë¶„í™”ëœ í† í°í™” ê²°ê³¼ ì„ íƒ (ì¼ë°˜ì ìœ¼ë¡œ Kkmaê°€ ê°€ì¥ ì„¸ë¶„í™”)
        src_tokens = kkma_tokens
    else:  # ì˜ì–´ ë° ê¸°íƒ€ ì–¸ì–´
        src_tokens = nltk.word_tokenize(original)

    # ëŒ€ìƒ ì–¸ì–´ í† í°í™”
    if dest_lang == "ko":  # í•œêµ­ì–´
        kkma = Kkma()  # ë” ì„¸ë¶„í™”ëœ í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì‚¬ìš©
        dest_tokens = kkma.morphs(translated)
    elif dest_lang == "ja":  # ì¼ë³¸ì–´
        mecab = MeCab.Tagger("-Owakati")
        dest_tokens = mecab.parse(translated).strip().split()
    else:  # ì˜ì–´ ë° ê¸°íƒ€ ì–¸ì–´
        dest_tokens = nltk.word_tokenize(translated)

    return src_tokens, dest_tokens

# í–¥ìƒëœ BLEU ì ìˆ˜ ê³„ì‚°
def calculate_bleu(original, translated, src_lang, dest_lang):
    translator = Translator()

    # ì—¬ëŸ¬ ë²ˆì—­ ëª¨ë¸ì„ í†µí•´ ë‹¤ì–‘í•œ ì°¸ì¡° ë²ˆì—­ ìƒì„±
    references = []

    # Google ë²ˆì—­
    try:
        google_ref = translator.translate(original, src=src_lang, dest=dest_lang).text
        references.append(google_ref)
    except Exception as e:
        print(f"Google Translate error: {e}")
        pass

    # Marian MT ëª¨ë¸ ì‚¬ìš© - ë‹¤ì–‘í•œ ì°¸ì¡° ë²ˆì—­ì„ ìœ„í•´
    try:
        model_name = f"Helsinki-NLP/opus-mt-{src_lang}-{dest_lang}"
        marian_tokenizer = MarianTokenizer.from_pretrained(model_name)
        marian_model = MarianMTModel.from_pretrained(model_name)

        inputs = marian_tokenizer(original, return_tensors="pt", padding=True, truncation=True)
        translated_tokens = marian_model.generate(**inputs)
        marian_ref = marian_tokenizer.decode(translated_tokens[0], skip_special_tokens=True)
        references.append(marian_ref)
    except Exception as e:
        # ëª¨ë¸ì´ ì—†ëŠ” ì–¸ì–´ ì¡°í•©ì¼ ê²½ìš° ê±´ë„ˆëœ€
        print(f"MarianMT error for {model_name}: {e}")
        pass

    # ì°¸ì¡° ë²ˆì—­ì´ í•˜ë‚˜ë„ ì—†ì„ ê²½ìš° (ìµœì†Œí•œ Google Translate ê²°ê³¼ë¼ë„ ì‚¬ìš©)
    if not references:
        # Fallback to Google Translate if MarianMT fails and no initial reference was added
        try:
            references = [translator.translate(original, src=src_lang, dest=dest_lang).text]
        except Exception as e:
            print(f"Failed to get any reference translations: {e}")
            return 0.0 # Return 0 if no reference can be generated

    # BLEU ê³„ì‚° ë°©ë²• ê°œì„  - SmoothingFunction ì ìš©
    # í† í°í™” í•¨ìˆ˜ ì •ì˜ (dest_langì— ë§ì¶° í† í°í™”)
    def get_tokens_for_bleu(text, lang):
        if lang == "ko":
            return Kkma().morphs(text)
        elif lang == "ja":
            return MeCab.Tagger("-Owakati").parse(text).strip().split()
        else:
            return nltk.word_tokenize(text)

    candidate = get_tokens_for_bleu(translated, dest_lang)
    references_tokenized = [get_tokens_for_bleu(ref, dest_lang) for ref in references]

    # NLTK BLEU (ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë” ì„¸ë°€í•œ í‰ê°€)
    weights = (0.25, 0.25, 0.25, 0.25)  # 1-gram, 2-gram, 3-gram, 4-gram
    smoothing = SmoothingFunction().method4  # smoothing ì ìš©

    nltk_bleu = sentence_bleu(references_tokenized, candidate, weights=weights, smoothing_function=smoothing) * 100

    # sacrebleu (ë§ë­‰ì¹˜ ë‹¨ìœ„ í‰ê°€)
    # sacrebleu expects references as a list of lists of strings, where each inner list is a reference for a sentence.
    # Since we're doing sentence-level evaluation, the structure is a bit different.
    # We provide `[translated]` as the hypotheses and `[[ref1], [ref2], ...]` as references for sacrebleu.
    sacre_bleu = sacrebleu.corpus_bleu([translated], references).score


    # ë‘ BLEU ì ìˆ˜ì˜ ê°€ì¤‘ í‰ê·  (ë¬¸ì¥ ë‹¨ìœ„ BLEUì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
    final_bleu = (nltk_bleu * 0.7) + (sacre_bleu * 0.3)

    return final_bleu

# í–¥ìƒëœ ë‹¨ì–´ ìˆ˜ ì²´í¬
def check_missing(original_tokens, translated_tokens, src_lang, dest_lang):
    # ì–¸ì–´ë³„ í† í° ë¹„ìœ¨ ê³„ì‚° (ì–¸ì–´ë§ˆë‹¤ í† í°í™” ì‹œ ë¹„ìœ¨ì´ ë‹¤ë¦„)
    lang_ratio = {
        "ko-en": 1.3,  # í•œêµ­ì–´ê°€ ì˜ì–´ë³´ë‹¤ í† í° ìˆ˜ê°€ ì ì„ ìˆ˜ ìˆìŒ
        "en-ko": 0.7,  # ì˜ì–´ê°€ í•œêµ­ì–´ë³´ë‹¤ í† í° ìˆ˜ê°€ ì ì„ ìˆ˜ ìˆìŒ
        "ja-en": 1.2,  # ì¼ë³¸ì–´ê°€ ì˜ì–´ë³´ë‹¤ í† í° ìˆ˜ê°€ ì ì„ ìˆ˜ ìˆìŒ
        "en-ja": 0.8,  # ì˜ì–´ê°€ ì¼ë³¸ì–´ë³´ë‹¤ í† í° ìˆ˜ê°€ ì ì„ ìˆ˜ ìˆìŒ
        "ko-ja": 1.1,  # í•œêµ­ì–´ê°€ ì¼ë³¸ì–´ë³´ë‹¤ í† í° ìˆ˜ê°€ ì ì„ ìˆ˜ ìˆìŒ
        "ja-ko": 0.9,  # ì¼ë³¸ì–´ê°€ í•œêµ­ì–´ë³´ë‹¤ í† í° ìˆ˜ê°€ ì ì„ ìˆ˜ ìˆìŒ
    }

    lang_pair = f"{src_lang}-{dest_lang}"
    ratio = lang_ratio.get(lang_pair, 1.0)  # ê¸°ë³¸ê°’ì€ 1.0

    expected_tokens = len(original_tokens) * ratio
    actual_tokens = len(translated_tokens)

    # í† í° ìˆ˜ ì°¨ì´ ê³„ì‚°
    difference = abs(expected_tokens - actual_tokens)
    difference_ratio = difference / expected_tokens if expected_tokens > 0 else 0

    if difference_ratio > 0.3:  # ì˜ˆìƒë˜ëŠ” í† í° ìˆ˜ì™€ 30% ì´ìƒ ì°¨ì´ë‚˜ë©´ ê²½ê³ 
        if expected_tokens > actual_tokens:
            return f"ë²ˆì—­ì—ì„œ ë‹¨ì–´ê°€ ëˆ„ë½ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. (ì˜ˆìƒ: {expected_tokens:.1f}, ì‹¤ì œ: {actual_tokens})"
        else:
            return f"ë²ˆì—­ì—ì„œ ë‹¨ì–´ê°€ ê³¼ë„í•˜ê²Œ ì¶”ê°€ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤. (ì˜ˆìƒ: {expected_tokens:.1f}, ì‹¤ì œ: {actual_tokens})"
    elif difference_ratio > 0.15:  # 15~30% ì°¨ì´
        if expected_tokens > actual_tokens:
            return f"ë²ˆì—­ì—ì„œ ì¼ë¶€ ë‹¨ì–´ê°€ ëˆ„ë½ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤. (ì˜ˆìƒ: {expected_tokens:.1f}, ì‹¤ì œ: {actual_tokens})"
        else:
            return f"ë²ˆì—­ì—ì„œ ì¼ë¶€ ë‹¨ì–´ê°€ ì¶”ê°€ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤. (ì˜ˆìƒ: {expected_tokens:.1f}, ì‹¤ì œ: {actual_tokens})"
    else:
        return f"ë‹¨ì–´ ìˆ˜ ì ì ˆ. (ì˜ˆìƒ: {expected_tokens:.1f}, ì‹¤ì œ: {actual_tokens})"

# ë¬¸ë²• ì˜¤ë¥˜ ì ê²€ ê¸°ëŠ¥ ì¶”ê°€
def check_grammar(translated, dest_lang):
    # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ë¬¸ë²• ê²€ì‚¬
    grammar_errors = 0
    error_messages = []

    if dest_lang == "en":
        # ì˜ì–´ ë¬¸ë²• ê·œì¹™ ê²€ì‚¬
        # ì£¼ì–´-ë™ì‚¬ ì¼ì¹˜ ê²€ì‚¬ (ê°„ë‹¨í•œ ì˜ˆ)
        subject_verb_errors = len(re.findall(r'\b(he|she|it) (are|am|were)\b', translated.lower()))
        grammar_errors += subject_verb_errors
        if subject_verb_errors > 0:
            error_messages.append("ì£¼ì–´-ë™ì‚¬ ë¶ˆì¼ì¹˜ ì˜¤ë¥˜")

        # ê´€ì‚¬ ì‚¬ìš© ê²€ì‚¬ (ê°„ë‹¨í•œ ì˜ˆ) - 'a' before a vowel sound
        article_errors = len(re.findall(r'\b(a) [aeiou]', translated.lower()))
        grammar_errors += article_errors
        if article_errors > 0:
            error_messages.append("ê´€ì‚¬ ì‚¬ìš© ì˜¤ë¥˜")

    elif dest_lang == "ko":
        # í•œêµ­ì–´ ë¬¸ë²• ê·œì¹™ ê²€ì‚¬ (ì¡°ì‚¬ ì˜¤ìš© ë“±)
        # ì´ ë¶€ë¶„ì€ ë§¤ìš° ë³µì¡í•˜ë©°, ê°„ë‹¨í•œ ì •ê·œì‹ìœ¼ë¡œëŠ” í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.
        # ì˜ˆì‹œë¡œ 'ì„ë¥¼' 'ì€ëŠ”' 'ì´ê°€' 'ì™€ê³¼' ê°€ ì—°ì†ìœ¼ë¡œ ë‚˜ì˜¤ê±°ë‚˜ ì–´ìƒ‰í•˜ê²Œ ì‚¬ìš©ëœ ê²½ìš°ë¥¼ ì°¾ì•„ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        particle_errors = len(re.findall(r'[ê°€-í£]+(ì„ë¥¼|ì€ëŠ”|ì´ê°€|ì™€ê³¼)\b', translated)) # This is a very simplistic check
        grammar_errors += particle_errors
        if particle_errors > 0:
            error_messages.append("ì¡°ì‚¬ ì˜¤ìš© ê°€ëŠ¥ì„±")

    elif dest_lang == "ja":
        # ì¼ë³¸ì–´ ë¬¸ë²• ê·œì¹™ ê²€ì‚¬ (ì¡°ì‚¬ ì˜¤ìš© ë“±)
        # ë§ˆì°¬ê°€ì§€ë¡œ ê°„ë‹¨í•œ ì •ê·œì‹ìœ¼ë¡œëŠ” í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.
        particle_errors = len(re.findall(r'[ã-ã‚“ã‚¡-ãƒ³]+(ã¯ãŒ|ã‚’ã«|ã«ã‚’)\b', translated)) # Simplistic check
        grammar_errors += particle_errors
        if particle_errors > 0:
            error_messages.append("ì¡°ì‚¬ ì˜¤ìš© ê°€ëŠ¥ì„±")

    grammar_score = max(0, 1 - (grammar_errors * 0.2)) # Each error reduces score by 0.2, min 0

    return grammar_score, ", ".join(error_messages) if error_messages else "ë¬¸ë²• ì˜¤ë¥˜ ì—†ìŒ"

# í–¥ìƒëœ ìì—°ìŠ¤ëŸ¬ì›€ ì ìˆ˜
def evaluate_naturalness(translated, lang):
    # ì–¸ì–´ë³„ ë¬¸ë²• ê²€ì‚¬
    grammar_score, grammar_errors = check_grammar(translated, lang)

    # ë¬¸ì¥ êµ¬ì¡° ë³µì¡ì„± ë¶„ì„
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]+', translated)
    sentences = [s.strip() for s in sentences if s.strip()] # Remove empty strings

    sentence_lengths = [len(s.split()) for s in sentences] # Word count per sentence
    num_sentences = len(sentences)
    avg_sentence_length = sum(sentence_lengths) / num_sentences if num_sentences > 0 else 0

    # í‰ê·  ë¬¸ì¥ ê¸¸ì´ì— ë”°ë¥¸ ì ìˆ˜ ì¡°ì • (ë„ˆë¬´ ê¸¸ê±°ë‚˜ ì§§ìœ¼ë©´ ê°ì )
    length_score = 1.0
    if lang == "en":
        if avg_sentence_length < 5 or avg_sentence_length > 35:
            length_score = 0.8
    elif lang == "ko":
        if avg_sentence_length < 10 or avg_sentence_length > 60:
            length_score = 0.8
    elif lang == "ja":
        if avg_sentence_length < 10 or avg_sentence_length > 50:
            length_score = 0.8

    # ë°˜ë³µ ë‹¨ì–´/êµ¬ë¬¸ ê²€ì‚¬
    tokens = nltk.word_tokenize(translated) if lang == "en" else tokenize_text("", translated, lang, lang)[1]
    if tokens:
        unique_tokens = set(tokens)
        repetition_ratio = len(unique_tokens) / len(tokens)
        repetition_score = min(1.0, repetition_ratio * 1.5)  # Repetition reduces score
    else:
        repetition_score = 1.0 # No tokens, so no repetition

    # ì „ì²´ ìì—°ìŠ¤ëŸ¬ì›€ ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ì¹˜ ì ìš©)
    naturalness_score = (grammar_score * 0.5) + (length_score * 0.3) + (repetition_score * 0.2)

    return naturalness_score, grammar_errors

# í–¥ìƒëœ ì˜ë¯¸ ìœ ì‚¬ë„ í‰ê°€
def evaluate_similarity(original, translated, src_lang, dest_lang):
    # ì–¸ì–´ë³„ ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸
    try:
        # ë‹¤êµ­ì–´ sentence-transformers ëª¨ë¸ ì‚¬ìš©
        model = SentenceTransformer('distiluse-base-multilingual-cased-v1')

        # ì›ë³¸ê³¼ ë²ˆì—­ë¬¸ì˜ ì„ë² ë”© ìƒì„±
        orig_embedding = model.encode([original])[0]
        trans_embedding = model.encode([translated])[0]

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = F.cosine_similarity(
            torch.tensor(orig_embedding).unsqueeze(0),
            torch.tensor(trans_embedding).unsqueeze(0)
        ).item()

        # í‚¤ì›Œë“œ ë³´ì¡´ ê²€ì‚¬ (ì¤‘ìš” ëª…ì‚¬ ë“±ì´ ë³´ì¡´ë˜ì—ˆëŠ”ì§€)
        src_tokens, _ = tokenize_text(original, "", src_lang, "")
        _, dest_tokens = tokenize_text("", translated, "", dest_lang)

        # ê³ ìœ  ëª…ì‚¬ ë° ì¤‘ìš” í‚¤ì›Œë“œ (ìˆ«ì, ì˜ë¬¸ ë“±) ì¶”ì¶œ
        # This part requires more sophisticated NLP for actual "keywords"
        # For now, a simple heuristic for alphanumeric tokens
        src_keywords = set([t for t in src_tokens if re.match(r'[A-Za-z0-9]', t) and len(t) > 1])
        dest_keywords = set([t for t in dest_tokens if re.match(r'[A-Za-z0-9]', t) and len(t) > 1])

        # Convert destination keywords to lowercase for comparison, as case might change
        dest_keywords_lower = {k.lower() for k in dest_keywords}

        # Keyword preservation score
        keyword_score = 1.0
        if src_keywords:
            preserved_count = 0
            for k in src_keywords:
                # Check if the keyword (or its lowercase version) is present in the translated tokens
                if any(k.lower() == d_lower for d_lower in dest_keywords_lower):
                    preserved_count += 1
                # Simple check for transliteration/translation (e.g., Apple -> ì• í”Œ)
                # This is very basic and would ideally need a translation dictionary or more advanced NLP
                elif src_lang == 'en' and dest_lang == 'ko' and k.lower() == 'apple' and 'ì• í”Œ' in translated:
                    preserved_count += 1
                elif src_lang == 'en' and dest_lang == 'ja' and k.lower() == 'apple' and 'ã‚¢ãƒƒãƒ—ãƒ«' in translated:
                    preserved_count += 1

            keyword_score = preserved_count / len(src_keywords)
        else:
            keyword_score = 1.0 # No keywords to check

        # ìµœì¢… ì˜ë¯¸ ìœ ì‚¬ë„ (ì„ë² ë”© ìœ ì‚¬ë„ 70%, í‚¤ì›Œë“œ ë³´ì¡´ 30%)
        final_similarity = (similarity * 0.7) + (keyword_score * 0.3)

        return final_similarity
    except Exception as e:
        print(f"ìœ ì‚¬ë„ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ìœ ì‚¬ë„ ë°˜í™˜
        return 0.5

# ì£¼ìš” ë²ˆì—­ ì˜¤ë¥˜ ê°ì§€
def detect_translation_errors(original, translated, src_lang, dest_lang):
    errors = []

    # ìˆ«ì ë¶ˆì¼ì¹˜ ê²€ì‚¬
    src_numbers = re.findall(r'\d+', original)
    dest_numbers = re.findall(r'\d+', translated)

    if len(src_numbers) != len(dest_numbers):
        errors.append("ìˆ«ì ê°œìˆ˜ ë¶ˆì¼ì¹˜")
    else:
        for i, num in enumerate(src_numbers):
            if i < len(dest_numbers) and num != dest_numbers[i]:
                errors.append(f"ìˆ«ì ê°’ ë¶ˆì¼ì¹˜: {num} â†’ {dest_numbers[i]}")

    # íŠ¹ìˆ˜ ê¸°í˜¸ ë¶ˆì¼ì¹˜ ê²€ì‚¬ (ì˜ˆ: ê´„í˜¸, ì¸ìš©ë¶€í˜¸ ë“±)
    # This might be too strict for some cases where punctuation can change due to language conventions
    src_punctuation = re.findall(r'[\[\]\(\)\{\}\"\'\Â«\Â»\â€\â€Ÿ\â€¹\â€º]', original)
    dest_punctuation = re.findall(r'[\[\]\(\)\{\}\"\'\Â«\Â»\â€\â€Ÿ\â€¹\â€º]', translated)

    if len(src_punctuation) != len(dest_punctuation):
        errors.append("íŠ¹ìˆ˜ ê¸°í˜¸ ê°œìˆ˜ ë¶ˆì¼ì¹˜ (ê´„í˜¸, ì¸ìš©ë¶€í˜¸ ë“±)")

    # ì´ë©”ì¼, URL ëˆ„ë½ ê²€ì‚¬
    src_emails = re.findall(r'[\w.+-]+@[\w-]+\.[\w.-]+', original)
    dest_emails = re.findall(r'[\w.+-]+@[\w-]+\.[\w.-]+', translated)

    if len(src_emails) != len(dest_emails):
        errors.append("ì´ë©”ì¼ ì£¼ì†Œ ëˆ„ë½ ë˜ëŠ” ë³€í˜•")
    else:
        # Check if actual email addresses are preserved (case-insensitive for comparison)
        src_emails_lower = {e.lower() for e in src_emails}
        dest_emails_lower = {e.lower() for e in dest_emails}
        if src_emails_lower != dest_emails_lower:
            errors.append("ì´ë©”ì¼ ì£¼ì†Œ ê°’ ë¶ˆì¼ì¹˜")


    src_urls = re.findall(r'https?://\S+', original)
    dest_urls = re.findall(r'https?://\S+', translated)

    if len(src_urls) != len(dest_urls):
        errors.append("URL ëˆ„ë½ ë˜ëŠ” ë³€í˜•")
    else:
        # Check if actual URLs are preserved
        src_urls_set = set(src_urls)
        dest_urls_set = set(dest_urls)
        if src_urls_set != dest_urls_set:
            errors.append("URL ê°’ ë¶ˆì¼ì¹˜")

    return errors

# ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
def calculate_overall_score(bleu_score, completeness_message, naturalness_score, similarity_score, errors):
    # ê° í•­ëª©ë³„ ê°€ì¤‘ì¹˜ ì„¤ì •
    weights = {
        'bleu': 0.2,
        'completeness': 0.2,
        'naturalness': 0.3,
        'similarity': 0.3
    }

    # ì™„ì „ì„± ì ìˆ˜ ê³„ì‚° (ëˆ„ë½ ë©”ì‹œì§€ì—ì„œ ì ìˆ˜ ì¶”ì¶œ)
    if "ì ì ˆ" in completeness_message:
        completeness_score = 1.0
    elif "ì¼ë¶€" in completeness_message:
        completeness_score = 0.7
    else: # "ë†’ìŠµë‹ˆë‹¤" or "ê³¼ë„í•˜ê²Œ"
        completeness_score = 0.4

    # ì˜¤ë¥˜ í˜ë„í‹°
    # More errors lead to a higher penalty, up to a certain maximum.
    error_penalty = min(0.2, len(errors) * 0.05) # Max 20% penalty for 4 or more errors

    # BLEU ì ìˆ˜ ì •ê·œí™” (0-100 -> 0-1)
    normalized_bleu = bleu_score / 100

    # ì¢…í•© ì ìˆ˜ ê³„ì‚°
    overall_score = (
        weights['bleu'] * normalized_bleu +
        weights['completeness'] * completeness_score +
        weights['naturalness'] * naturalness_score +
        weights['similarity'] * similarity_score
    ) * (1 - error_penalty) # Apply penalty at the end

    # ìµœì¢… 100ì  ë§Œì ìœ¼ë¡œ ë³€í™˜
    final_score = overall_score * 100

    # ì ìˆ˜ì— ë”°ë¥¸ ë“±ê¸‰ ë¶€ì—¬
    if final_score >= 90:
        grade = "A+ (ì „ë¬¸ ë²ˆì—­ì‚¬ ìˆ˜ì¤€)"
    elif final_score >= 80:
        grade = "A (ë§¤ìš° ìš°ìˆ˜)"
    elif final_score >= 70:
        grade = "B (ìš°ìˆ˜)"
    elif final_score >= 60:
        grade = "C (ì–‘í˜¸)"
    elif final_score >= 50:
        grade = "D (ê°œì„  í•„ìš”)"
    else:
        grade = "F (ë¶ˆëŸ‰)"

    return round(final_score, 1), grade

# ë©”ì¸ í‰ê°€ í•¨ìˆ˜
def evaluate_translation(original, translated, src_lang, dest_lang):
    print("\ní‰ê°€ ì§„í–‰ ì¤‘...")

    # í† í°í™”
    src_tokens, dest_tokens = tokenize_text(original, translated, src_lang, dest_lang)

    # BLEU ì ìˆ˜ ê³„ì‚°
    bleu_score = calculate_bleu(original, translated, src_lang, dest_lang)
    print("- BLEU ì ìˆ˜ ê³„ì‚° ì™„ë£Œ")

    # ì™„ì „ì„± ê²€ì‚¬
    completeness_message = check_missing(src_tokens, dest_tokens, src_lang, dest_lang)
    print("- ì™„ì „ì„± ê²€ì‚¬ ì™„ë£Œ")

    # ë¬¸ë²• ë° ìì—°ìŠ¤ëŸ¬ì›€ í‰ê°€
    naturalness_score, grammar_errors = evaluate_naturalness(translated, dest_lang)
    print("- ìì—°ìŠ¤ëŸ¬ì›€ í‰ê°€ ì™„ë£Œ")

    # ì˜ë¯¸ ìœ ì‚¬ë„ í‰ê°€
    similarity_score = evaluate_similarity(original, translated, src_lang, dest_lang)
    print("- ì˜ë¯¸ ìœ ì‚¬ë„ í‰ê°€ ì™„ë£Œ")

    # ë²ˆì—­ ì˜¤ë¥˜ ê°ì§€
    translation_errors = detect_translation_errors(original, translated, src_lang, dest_lang)
    print("- ë²ˆì—­ ì˜¤ë¥˜ ê°ì§€ ì™„ë£Œ")

    # ì¢…í•© ì ìˆ˜ ê³„ì‚°
    overall_score, grade = calculate_overall_score(
        bleu_score,
        completeness_message,
        naturalness_score,
        similarity_score,
        translation_errors
    )

    return {
        "bleu_score": bleu_score,
        "completeness": completeness_message,
        "naturalness": naturalness_score,
        "grammar_errors": grammar_errors,
        "similarity": similarity_score,
        "translation_errors": translation_errors,
        "overall_score": overall_score,
        "grade": grade
    }

# ê²°ê³¼ ì‹œê°ì  í‘œì‹œ
def display_results(results):
    print("\n" + "="*50)
    print("ğŸ“Š ë²ˆì—­ í’ˆì§ˆ í‰ê°€ ê²°ê³¼")
    print("="*50)

    print(f"\nğŸ† ì¢…í•© ì ìˆ˜: {results['overall_score']}/100 ({results['grade']})")

    print("\nğŸ“ˆ ì„¸ë¶€ í‰ê°€:")
    print(f"- BLEU ì ìˆ˜: {results['bleu_score']:.2f}/100")
    print(f"- ì™„ì „ì„±: {results['completeness']}")
    print(f"- ìì—°ìŠ¤ëŸ¬ì›€: {results['naturalness']:.2f}/1.0")
    print(f"- ë¬¸ë²• ê²€ì‚¬: {results['grammar_errors']}")
    print(f"- ì˜ë¯¸ ìœ ì‚¬ë„: {results['similarity']:.2f}/1.0")

    if results['translation_errors']:
        print("\nâš ï¸ ê°ì§€ëœ ì˜¤ë¥˜:")
        for error in results['translation_errors']:
            print(f"  â€¢ {error}")
    else:
        print("\nâœ… ì¤‘ëŒ€í•œ ë²ˆì—­ ì˜¤ë¥˜ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    print("\nğŸ’¡ ê°œì„  ì œì•ˆ:")
    if results['overall_score'] < 60:
        print("  â€¢ ë²ˆì—­ì˜ ì •í™•ì„±ê³¼ ì™„ì „ì„±ì„ ë†’ì´ëŠ” ê²ƒì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print("  â€¢ ì›ë¬¸ì˜ ì˜ë¯¸ë¥¼ ë” ì˜ ì „ë‹¬í•  ìˆ˜ ìˆë„ë¡ ë²ˆì—­ì„ ì¬ê²€í† í•˜ì„¸ìš”.")
    elif results['overall_score'] < 80:
        print("  â€¢ ë¬¸ë²•ê³¼ ìì—°ìŠ¤ëŸ¬ì›€ì„ ê°œì„ í•˜ë©´ ë²ˆì—­ í’ˆì§ˆì´ í–¥ìƒë  ê²ƒì…ë‹ˆë‹¤.")
        print("  â€¢ ì¼ë¶€ ë¬¸ì¥ êµ¬ì¡°ë¥¼ ëŒ€ìƒ ì–¸ì–´ì— ë§ê²Œ ìˆ˜ì •í•´ë³´ì„¸ìš”.")
    else:
        print("  â€¢ ë†’ì€ í’ˆì§ˆì˜ ë²ˆì—­ì…ë‹ˆë‹¤. ë¯¸ì„¸í•œ ì¡°ì •ë§Œ í•„ìš”í•©ë‹ˆë‹¤.")

    print("="*50)

# ë©”ì¸ í•¨ìˆ˜
def main():
    print("\nğŸŒ ê³ ê¸‰ AI ë²ˆì—­ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ ğŸŒ")
    print("-"*50)

    src_lang = input("ì›ë¬¸ ì–¸ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ja: ì¼ë³¸ì–´, ko: í•œêµ­ì–´, en: ì˜ì–´): ").strip().lower()
    dest_lang = input("ë²ˆì—­ ì–¸ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ja: ì¼ë³¸ì–´, ko: í•œêµ­ì–´, en: ì˜ì–´): ").strip().lower()

    print("\nì›ë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")
    original = input().strip()

    print("\në²ˆì—­ë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")
    translated = input().strip()

    print("\ní‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    results = evaluate_translation(original, translated, src_lang, dest_lang)
    display_results(results)

if __name__ == "__main__":
    main()
